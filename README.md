# ğŸ¤– Robot Hallway MDP - Interactive Reinforcement Learning Simulator

An interactive visualization tool for understanding Markov Decision Processes (MDPs) through a simple robot navigation problem. This project demonstrates both the theoretical algorithms and practical simulation of reinforcement learning concepts.

---
<img width="1887" height="1053" alt="Image" src="https://github.com/user-attachments/assets/0cf2aa54-48e4-4581-965b-74e0ac969b3e" />

<img width="1894" height="1079" alt="Image" src="https://github.com/user-attachments/assets/c7069811-8ef0-4ae8-935e-d85557a43616" />

---
## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Problem Description](#problem-description)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [MDP Formulation](#mdp-formulation)
- [Algorithms](#algorithms)
- [Project Structure](#project-structure)
- [Scenarios](#scenarios)
- [Technologies](#technologies)

## ğŸ¯ Overview

This project implements a **Markov Decision Process (MDP)** solver with an interactive React-based visualization. A robot navigates a 1D hallway with 4 positions, aiming to reach a charging station while dealing with uncertain movements and various environmental challenges.

The project includes:
- âœ… **Python implementation** (Jupyter Notebook) - Core MDP algorithms
- âœ… **React web application** - Interactive visualization and simulation
- âœ… **Multiple scenarios** - Different reward structures and obstacles
- âœ… **Two algorithms** - Value Iteration and Policy Iteration

## ğŸš€ Problem Description

### Scenario
A robot moves along a hallway with 4 positions: `[0, 1, 2, 3]`

**Goal:** Reach the charging station at position 3

### MDP Components

#### States (S)
The robot's position: `S = {0, 1, 2, 3}`

#### Actions (A)
At each step, the robot can choose:
- `LEFT` - Attempt to move left
- `RIGHT` - Attempt to move right

#### Transition Probabilities (T)
Motion is **stochastic** (noisy):
- **Intended action succeeds:** 80% probability
- **Robot stays in place:** 20% probability (slips)
- Boundaries are enforced (cannot move outside [0,3])

#### Rewards (R)
- **Reaching charging station (state 3):** +10
- **Every other move:** -1 (step cost)
- **Terminal state:** State 3 (episode ends)

#### Discount Factor (Î³)
- Default: 0.9 (adjustable in web app)
- Balances immediate vs. future rewards

## âœ¨ Features

### Interactive Web Application
- ğŸ® **Real-time simulation** - Watch the robot navigate using the optimal policy
- ğŸ¨ **Visual feedback** - Color-coded states (goals, hazards, rewards)
- ğŸ“Š **Algorithm comparison** - Switch between Value Iteration and Policy Iteration
- âš™ï¸ **Configurable parameters** - Adjust discount factor and animation speed
- ğŸ“ˆ **Performance metrics** - Track steps, rewards, and stuck instances
- ğŸ¯ **Multiple scenarios** - 6 pre-built scenarios with different challenges
- ğŸ“ **Movement history** - Detailed log of robot's actions and outcomes

### Python Implementation
- ğŸ **Clean NumPy implementation** - Easy to understand and modify
- ğŸ““ **Jupyter Notebook** - Step-by-step explanation with outputs
- ğŸ”„ **Simulation testing** - Verify policy from different starting positions
- ğŸ“Š **Value convergence** - Visualization of learning process

## ğŸ“¦ Installation

### Prerequisites
- Node.js (v14 or higher)
- npm or yarn
- Python 3.7+ (for notebook)

### React Application Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/robot-hallway-mdp.git
cd robot-hallway-mdp
```

2. **Install dependencies**
```bash
npm install
```

3. **Install Tailwind CSS**
```bash
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

4. **Configure Tailwind**

Create/edit `tailwind.config.js`:
```javascript
module.exports = {
  content: ["./src/**/*.{js,jsx,ts,tsx}"],
  theme: { extend: {} },
  plugins: [],
}
```

5. **Update CSS**

In `src/index.css`:
```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

6. **Start the application**
```bash
npm start
```

The app will open at `http://localhost:3000`

### Python Notebook Setup

1. **Install required packages**
```bash
pip install numpy jupyter
```

2. **Launch Jupyter**
```bash
jupyter notebook MDP.ipynb
```

## ğŸ® Usage

### Web Application

1. **Select a scenario** from the scenario selector
2. **Configure settings** (optional):
   - Adjust discount factor (Î³)
   - Change animation speed
   - Switch between algorithms
3. **Choose starting position** (0, 1, or 2)
4. **Click "Start"** to watch the robot navigate
5. **Observe**:
   - Optimal policy (Ï€*) for each state
   - Q-values (state-action values)
   - Real-time statistics
   - Movement history

### Python Implementation

Run cells sequentially in the Jupyter notebook:
1. **Define MDP components** (states, actions, transitions, rewards)
2. **Run Value Iteration** algorithm
3. **Extract optimal policy**
4. **Simulate** robot behavior from different starting positions

## ğŸ§® MDP Formulation

### Mathematical Representation

**State Value Function:**
```
V(s) = max_a Î£ P(s'|s,a)[R(s,a,s') + Î³V(s')]
```

**Optimal Policy:**
```
Ï€*(s) = argmax_a Î£ P(s'|s,a)[R(s,a,s') + Î³V(s')]
```

**Q-Value (Action-Value Function):**
```
Q(s,a) = Î£ P(s'|s,a)[R(s,a,s') + Î³V(s')]
```

### Example Results

**Optimal Values (Î³=0.9):**
```
State 0: V(0) = 5.04
State 1: V(1) = 7.13
State 2: V(2) = 9.51
State 3: V(3) = 0.00 (terminal)
```

**Optimal Policy:**
```
State 0: RIGHT
State 1: RIGHT
State 2: RIGHT
State 3: TERMINAL
```

## ğŸ”„ Algorithms

### 1. Value Iteration

**Bellman Optimality Update:**
```python
V(s) â† max_a Î£ P(s'|s,a)[R(s,a,s') + Î³V(s')]
```

**Convergence:** When `max|V_new(s) - V_old(s)| < Î¸`

**Complexity:** O(|S|Â²|A|) per iteration

### 2. Policy Iteration

**Two phases:**
1. **Policy Evaluation:** Compute V^Ï€ for current policy Ï€
2. **Policy Improvement:** Update Ï€ to be greedy w.r.t. V^Ï€

**Convergence:** Typically faster than Value Iteration in practice

## ğŸ“‚ Project Structure

```
robot-hallway-mdp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.js                 # Main React component
â”‚   â”œâ”€â”€ index.js               # Entry point
â”‚   â””â”€â”€ index.css              # Tailwind imports
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ MDP.ipynb                  # Python implementation
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.js
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ¯ Scenarios

### 1. Goal on Right âš¡
- **Goal:** Position 3 (+10)
- **Challenge:** Navigate from left to right

### 2. Goal on Left âš¡
- **Goal:** Position 0 (+10)
- **Challenge:** Navigate from right to left

### 3. Middle Treasure ğŸ’
- **Rewards:** Position 1 (+5), Position 3 (+10)
- **Challenge:** Decide whether to collect treasure first

### 4. Avoid Hazard â˜ ï¸
- **Goal:** Position 3 (+10)
- **Hazard:** Position 2 (-8)
- **Challenge:** High penalty zone - risk vs. reward

### 5. Two Goals ğŸ¯
- **Rewards:** Position 0 (+3), Position 3 (+10)
- **Challenge:** Choose between nearby small reward or distant large reward

### 6. Expensive Path ğŸ’°
- **Goal:** Position 3 (+10)
- **Cost:** Position 2 (-5)
- **Challenge:** Expensive intermediate state affects optimal policy

## ğŸ› ï¸ Technologies

### Frontend
- **React 18** - UI framework
- **Tailwind CSS** - Styling
- **Lucide React** - Icons
- **JavaScript ES6+** - Core language

### Backend/Algorithm
- **Python 3** - Algorithm implementation
- **NumPy** - Numerical computations
- **Jupyter** - Interactive notebooks

## ğŸ“Š Performance Metrics

The application tracks:
- **Steps Taken** - Number of moves to reach goal
- **Total Reward** - Cumulative reward collected
- **Stuck Count** - Times robot failed to move (20% probability)
- **Convergence Time** - Algorithm computation time in milliseconds
- **Iterations** - Number of iterations until convergence

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new scenarios
- Improve algorithms
- Enhance UI/UX

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author

Created as an educational tool for understanding Markov Decision Processes and Reinforcement Learning fundamentals.

## ğŸ“š References

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*
- Bellman, R. (1957). *Dynamic Programming*
- Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach*

## ğŸ“ Educational Use

This project is ideal for:
- Computer Science students learning RL
- AI/ML course demonstrations
- Self-study and experimentation
- Teaching MDP concepts interactively

---

**â­ If you find this project helpful, please star the repository!**

## ğŸ› Known Issues

- None currently reported

## ğŸ”® Future Enhancements

- [ ] Add Q-Learning visualization
- [ ] Implement SARSA algorithm
- [ ] 2D grid world environment
- [ ] Policy gradient methods
- [ ] Deep Q-Network (DQN) comparison
- [ ] Export simulation data

---

**Made with â¤ï¸ for Reinforcement Learning Education**
